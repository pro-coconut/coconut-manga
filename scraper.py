#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import requests
from bs4 import BeautifulSoup
import json
import os
import time
import re
from urllib.parse import urljoin, urlparse

# ========== C·∫§U H√åNH ==========
BASE_URL = "https://nettruyen0209.com"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36",
    "Accept-Language": "vi-VN,vi;q=0.9,en;q=0.8"
}

DELAY = 1.5                # Delay gi·ªØa requests (s·ª≠a n·∫øu c·∫ßn)
MAX_LISTING_PAGES = 20     # S·ªë trang listing s·∫Ω qu√©t (tƒÉng n·∫øu mu·ªën qu√©t to√†n site)
MAX_STORIES = None         # N·∫øu mu·ªën gi·ªõi h·∫°n t·ªïng s·ªë truy·ªán (None = kh√¥ng gi·ªõi h·∫°n)
OUTPUT_JSON = "stories.json"

# ========== TI·ªÜN √çCH ==========
def safe_get(url, **kwargs):
    """G·ªçi requests.get c√≥ x·ª≠ l√Ω l·ªói c∆° b·∫£n."""
    try:
        res = requests.get(url, headers=HEADERS, timeout=20, **kwargs)
        res.raise_for_status()
        return res
    except Exception as e:
        print(f"‚ö† L·ªói GET {url}: {e}")
        return None

def normalize_link(href):
    if not href:
        return None
    return urljoin(BASE_URL, href)

def is_manga_link(href):
    if not href:
        return False
    # nettruyen0209 d√πng ƒë∆∞·ªùng d·∫´n nh∆∞ /manga/slug...
    return "/manga/" in href and href.count("/") >= 2

# ========== L·∫§Y DANH S√ÅCH TRUY·ªÜN T·ª™ C√ÅC TRANG LISTING ==========
def gather_manga_links(max_pages=MAX_LISTING_PAGES, max_stories=None):
    """
    Qu√©t nhi·ªÅu trang (b·∫Øt ƒë·∫ßu t·ª´ trang ch·ªß) ƒë·ªÉ t√¨m c√°c link ch·ª©a '/manga/'.
    Tr·∫£ v·ªÅ list link duy nh·∫•t (kh√¥ng duplicate).
    """
    found = []
    seen = set()
    seeds = [BASE_URL, urljoin(BASE_URL, "/truyen-tranh"), urljoin(BASE_URL, "/truyen-tranh?page=1")]
    # M·ªôt s·ªë trang c√≥ /page/2 ho·∫∑c ?page=2; ch√∫ng ta th·ª≠ c·∫£ hai m·∫´u
    page_variants = [
        lambda n: urljoin(BASE_URL, f"/page/{n}"),
        lambda n: urljoin(BASE_URL, f"/?page={n}"),
        lambda n: urljoin(BASE_URL, f"/truyen-tranh?page={n}"),
        lambda n: urljoin(BASE_URL, f"/truyen-tranh/page/{n}")
    ]

    # Kh·ªüi ƒë·∫ßu: qu√©t seeds
    for seed in seeds:
        print("Qu√©t seed:", seed)
        r = safe_get(seed)
        if not r:
            continue
        soup = BeautifulSoup(r.text, "html.parser")
        a_tags = soup.find_all("a", href=True)
        for a in a_tags:
            href = a.get("href")
            if is_manga_link(href):
                full = normalize_link(href)
                if full not in seen:
                    seen.add(full)
                    found.append(full)
                    print("  ‚Üí T√¨m:", full)
                    if max_stories and len(found) >= max_stories:
                        return found

    # Qu√©t th√™m theo m·∫´u page
    for n in range(1, max_pages + 1):
        for variant in page_variants:
            url = variant(n)
            print(f"Qu√©t trang listing: {url}")
            r = safe_get(url)
            if not r:
                continue
            soup = BeautifulSoup(r.text, "html.parser")
            a_tags = soup.find_all("a", href=True)
            added = 0
            for a in a_tags:
                href = a.get("href")
                if is_manga_link(href):
                    full = normalize_link(href)
                    if full not in seen:
                        seen.add(full)
                        found.append(full)
                        added += 1
                        print("  ‚Üí T√¨m:", full)
                        if max_stories and len(found) >= max_stories:
                            return found
            print(f"  ‚Üí Th√™m {added} link t·ª´ trang {url}")
            time.sleep(DELAY)
        # ng·∫Øt s·ªõm n·∫øu kh√¥ng c√≥ link m·ªõi trong v√≤ng 1 v√≤ng page_variants
    print(f"T·ªïng link thu th·∫≠p: {len(found)}")
    return found

# ========== L·∫§Y TO√ÄN B·ªò CHAPTER TRONG 1 TRUY·ªÜN ==========
def parse_story_page(story_url):
    """
    Tr·∫£ v·ªÅ dict:
    {
      id, title, author, description, thumbnail, chapters: [{name, url}, ...]
    }
    """
    r = safe_get(story_url)
    if not r:
        return None
    soup = BeautifulSoup(r.text, "html.parser")

    # Title
    title_elem = soup.select_one("h1.title-detail") or soup.select_one("h1")
    title = title_elem.get_text(strip=True) if title_elem else "Kh√¥ng c√≥ ti√™u ƒë·ªÅ"
    sid = re.sub(r'[^a-z0-9]', '', title.lower())

    # Author (c·ªë g·∫Øng l·∫•y)
    author = "Kh√¥ng r√µ"
    try:
        # t√¨m th·∫ª ch·ª©a "T√°c gi·∫£"
        li = soup.find(lambda tag: tag.name in ["li", "p", "div", "span"] and "T√°c gi·∫£" in tag.get_text())
        if li:
            a = li.find("a")
            if a:
                author = a.get_text(strip=True)
    except Exception:
        pass

    # Description
    desc = ""
    desc_elem = soup.select_one(".detail-content, .summary, .desc, .story-intro")
    if desc_elem:
        desc = desc_elem.get_text(strip=True)

    # Thumbnail
    thumb = ""
    img_elem = soup.select_one(".col-image img, .book img, .thumb img, img[itemprop='image']")
    if img_elem:
        thumb = img_elem.get("src") or img_elem.get("data-src") or ""

    # Danh s√°ch chapter (nhi·ªÅu selector ƒë·ªÉ b·ªÅn)
    chap_selectors = [
        ".list-chapter li a",
        "ul.row-content-chapter li a",
        ".chapter_list a",
        ".chapters a",
        ".chapter a",
        ".chapter-list a",
        "a[href*='/manga/'][href*='chapter']"  # fallback: link ch·ª©a 'chapter'
    ]

    chap_links = []
    for sel in chap_selectors:
        elems = soup.select(sel)
        if elems:
            for a in elems:
                href = a.get("href")
                name = a.get_text(strip=True)
                if href and name:
                    full = normalize_link(href)
                    # lo·∫°i ra c√°c link kh√¥ng ph·∫£i chapter
                    if "chapter" in full or re.search(r'chap(ter)?[-\s\d]', full, re.I):
                        chap_links.append((name, full))
            if chap_links:
                break

    # N·∫øu ch∆∞a t√¨m ƒë∆∞·ª£c chapter b·∫±ng selectors, t√¨m t·∫•t c·∫£ a ch·ª©a 'chapter' trong href
    if not chap_links:
        for a in soup.find_all("a", href=True):
            href = a["href"]
            if "chapter" in href.lower() or re.search(r'/chapter[-_/]?\d+', href, re.I):
                name = a.get_text(strip=True) or href.split("/")[-1]
                chap_links.append((name, normalize_link(href)))

    # Deduplicate v√† sort theo th·ª© t·ª± xu·∫•t hi·ªán (sau ƒë√≥ ƒë·∫£o ƒë·ªÉ chap1->chapN)
    seen = set()
    cleaned = []
    for name, link in chap_links:
        if link not in seen:
            seen.add(link)
            cleaned.append({"name": name, "url": link})

    # M·ªôt s·ªë trang li·ªát k√™ chapter theo th·ª© t·ª± m·ªõi->c≈©, n√™n ƒë·∫£o ƒë·ªÉ chap 1 tr∆∞·ªõc
    cleaned.reverse()

    story = {
        "id": sid,
        "title": title,
        "author": author,
        "description": desc,
        "thumbnail": thumb,
        "chapters": cleaned
    }
    print(f"Parsed story: {title} - {len(cleaned)} chapters found")
    return story

# ========== L·∫§Y ·∫¢NH TRONG 1 CHAPTER ==========
def parse_chapter_images(chap_url):
    """
    Tr·∫£ v·ªÅ list url ·∫£nh (chu·ªói) cho chapter.
    """
    r = safe_get(chap_url)
    if not r:
        return []
    soup = BeautifulSoup(r.text, "html.parser")

    # Nhi·ªÅu selector cho ·∫£nh
    img_selectors = [
        ".reading-detail img",   # m·ªôt s·ªë theme
        ".chapter-content img",
        ".page-chapter img",
        "img.img-responsive",
        ".container-chapter-reader img",
        "img"
    ]

    imgs = []
    for sel in img_selectors:
        elems = soup.select(sel)
        if not elems:
            continue
        for img in elems:
            src = img.get("src") or img.get("data-src") or img.get("data-original") or ""
            if src and src.startswith("http"):
                imgs.append(src)
        if imgs:
            break

    # Filter ra c√°c ·∫£nh c√≥ k√≠ch th∆∞·ªõc, kh√¥ng ph·∫£i icon
    filtered = []
    for u in imgs:
        # b·ªè c√°c link favicon ho·∫∑c small icons
        if any(x in u for x in ["/logo", "favicon", "icons", "icon"]):
            continue
        filtered.append(u)
    print(f"    ‚Üí {len(filtered)} images found in chapter {chap_url}")
    return filtered

# ========== C·∫¨P NH·∫¨T STORIES.JSON ==========
def load_stories():
    if not os.path.exists(OUTPUT_JSON):
        with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
            json.dump([], f, ensure_ascii=False, indent=2)
    with open(OUTPUT_JSON, "r", encoding="utf-8") as f:
        try:
            return json.load(f)
        except Exception:
            return []

def save_stories(stories):
    with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
        json.dump(stories, f, ensure_ascii=False, indent=2)

def merge_and_save(parsed_story):
    """
    parsed_story c√≥ d·∫°ng: id, title, author, description, thumbnail, chapters: [{name, url}, ...]
    Ta c·∫ßn: l∆∞u c·∫•u tr√∫c v·ªõi chapters ch·ª©a {name, images: [...]}
    """
    if not parsed_story:
        return

    stories = load_stories()
    exist = next((s for s in stories if s.get("id") == parsed_story["id"]), None)

    if not exist:
        # Truy·ªán m·ªõi: t·∫£i t·ª´ng chapter images v√† th√™m to√†n b·ªô
        print(f"üü¢ Th√™m truy·ªán m·ªõi: {parsed_story['title']}")
        story_entry = {
            "id": parsed_story["id"],
            "title": parsed_story["title"],
            "author": parsed_story.get("author", ""),
            "description": parsed_story.get("description", ""),
            "thumbnail": parsed_story.get("thumbnail", ""),
            "chapters": []
        }
        for chap in parsed_story["chapters"]:
            print(f"   T·∫£i chapter: {chap['name']}")
            images = parse_chapter_images(chap["url"])
            story_entry["chapters"].append({
                "name": chap["name"],
                "images": images
            })
            time.sleep(DELAY)
        stories.append(story_entry)
        save_stories(stories)
    else:
        # Truy·ªán ƒë√£ t·ªìn t·∫°i: ch·ªâ th√™m nh·ªØng chapter ch∆∞a c√≥
        print(f"üîç Truy·ªán ƒë√£ c√≥: {parsed_story['title']}, ki·ªÉm tra chapter m·ªõi...")
        existing_names = {c["name"] for c in exist.get("chapters", [])}
        added = 0
        for chap in parsed_story["chapters"]:
            if chap["name"] not in existing_names:
                print(f"   Th√™m chapter m·ªõi: {chap['name']}")
                images = parse_chapter_images(chap["url"])
                exist["chapters"].append({"name": chap["name"], "images": images})
                added += 1
                time.sleep(DELAY)
        if added:
            save_stories(stories)
        print(f"   ƒê√£ th√™m {added} chapter m·ªõi")

# ========== MAIN ==========
def main():
    print("=== BOT NETTRUYEN0209 START ===")
    links = gather_manga_links(max_pages=MAX_LISTING_PAGES, max_stories=MAX_STORIES)
    print(f"‚û° T·ªïng truy·ªán s·∫Ω x·ª≠ l√Ω: {len(links)}")

    # Duy·ªát c√°c truy·ªán
    for idx, link in enumerate(links, start=1):
        print(f"\n[{idx}/{len(links)}] X·ª≠ l√Ω: {link}")
        parsed = parse_story_page(link)
        if not parsed:
            print("  ! B·ªè qua do l·ªói parse")
            continue
        merge_and_save(parsed)
        # delay gi·ªØa truy·ªán ƒë·ªÉ tr√°nh b·ªã ch·∫∑n
        time.sleep(DELAY)

    print("\n=== BOT HO√ÄN TH√ÄNH ===")

if __name__ == "__main__":
    main()
